# ROS related
import rclpy
from rclpy.node import Node
from llm_interfaces.srv import ChatGPT
from std_msgs.msg import String
from geometry_msgs.msg import Twist
from threading import Lock  # å¯¼å…¥çº¿ç¨‹é”
from collections import deque
from typing import Deque, Optional
import copy
# LLM related
import json
import os
import time
import openai
from llm_config.user_config import UserConfig
from openai import OpenAI
import threading

# Global Initialization
config = UserConfig()
openai.api_key = config.openai_api_key
openai.api_base = config.openai_api_base


class ChatGPTNode(Node):
    def __init__(self):
        super().__init__("ChatGPT_node")
        # åˆå§‹åŒ–å‘å¸ƒè€…ã€è®¢é˜…è€…å’Œå®¢æˆ·ç«¯
        self.initialization_publisher = self.create_publisher(String, "/llm_initialization_state", 0)
        self.llm_state_publisher = self.create_publisher(String, "/llm_state", 0)
        self.llm_state_subscriber = self.create_subscription(String, "/llm_state", self.state_listener_callback, 0)
        self.llm_input_subscriber = self.create_subscription(String, "/llm_input_audio_to_text", self.llm_callback, 0)
        self.llm_response_type_publisher = self.create_publisher(String, "/llm_response_type", 0)
        self.llm_feedback_publisher = self.create_publisher(String, "/llm_feedback_to_user", 0)
        self.function_call_client = self.create_client(ChatGPT, "/ChatGPT_function_call_service")
        self.function_call_request = ChatGPT.Request()

        # åˆå§‹åŒ–å¤šå°è½¦å‘å¸ƒè€…
        self.cmd_vel_publishers = {}

        self.get_logger().info("ChatGPT Function Call Server is ready")
        self.output_publisher = self.create_publisher(String, "ChatGPT_text_output", 10)
        self.start_timestamp = time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime())
        self.chat_history_file = os.path.join(config.chat_history_path, f"chat_history_{self.start_timestamp}.json")
        self.write_chat_history_to_json()
        self.get_logger().info(f"Chat history saved to {self.chat_history_file}")
        self.publish_string("llm_model_processing", self.initialization_publisher)
        self.client = OpenAI(base_url=config.openai_api_base, api_key=config.openai_api_key)

        # æ–°å¢ä»»åŠ¡é˜Ÿåˆ—å’ŒçŠ¶æ€å˜é‡
        self.turtle_status = "IDLE"
        self.task_fail_counts = {}  # æ–°å¢å¤±è´¥è®¡æ•°å™¨
        self.motion_queue = deque(maxlen=10)  # æ›¿æ¢listä¸ºdequeï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
        self.current_request = None  # ä¿®æ”¹ç‚¹ï¼šåˆå§‹åŒ– current_request å±æ€§
        self.status_subscriber = self.create_subscription(
            String, '/turtle_robot/status', self.status_callback, 10)
        self.motion_queue_lock = threading.Lock()  # æ–°å¢ä»»åŠ¡é˜Ÿåˆ—é”

    def status_callback(self, msg):
        """æœºå™¨äººçŠ¶æ€å›è°ƒå‡½æ•°ï¼ˆçº¿ç¨‹å®‰å…¨+dequeä¼˜åŒ–ç‰ˆï¼‰"""
        with self.motion_queue_lock:
            # åŸå­æ“ä½œï¼šçŠ¶æ€æ›´æ–°ä¸é˜Ÿåˆ—è®¿é—®
            old_status = self.turtle_status
            self.turtle_status = msg.data  # æ›´æ–°å½“å‰çŠ¶æ€

            # çŠ¶æ€å˜æ›´æ—¥å¿—ï¼ˆéå…³é”®æ“ä½œï¼Œä¸å ç”¨é”ï¼‰
            if old_status != self.turtle_status:
                self.get_logger().info(f"çŠ¶æ€å˜æ›´: {old_status} -> {self.turtle_status}")

            # ä»…åœ¨IDLEçŠ¶æ€å¤„ç†é˜Ÿåˆ—
            if self.turtle_status == "IDLE" and self.motion_queue:
                # å…³é”®ä¿®æ­£ç‚¹ï¼šä½¿ç”¨popleft()
                next_task = self.motion_queue.popleft()  # ğŸš© dequeçš„æ­£ç¡®å–ä»»åŠ¡æ–¹å¼

                # é˜²å¾¡æ€§ç¼–ç¨‹ï¼šæ•°æ®æ ¡éªŒ
                if not isinstance(next_task, str):
                    self.get_logger().error(f"éæ³•ä»»åŠ¡ç±»å‹: {type(next_task)}")
                    return

                # å¤±è´¥æ¬¡æ•°æ£€æŸ¥
                if self.task_fail_counts.get(next_task, 0) >= 3:
                    self.get_logger().warn(f"ä»»åŠ¡é‡è¯•è¶…é™: {next_task[:20]}...")
                    del self.task_fail_counts[next_task]
                    return

                # å‘é€ä»»åŠ¡ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
                try:
                    self.send_motion_request(next_task)
                except Exception as e:
                    self.get_logger().error(f"ä»»åŠ¡å‘é€å¤±è´¥: {str(e)}")
                    # é‡è¯•é€»è¾‘
                    self.motion_queue.appendleft(next_task)  # ğŸš© ä½¿ç”¨dequeä¸“æœ‰æ–¹æ³•
                    self.task_fail_counts[next_task] = self.task_fail_counts.get(next_task, 0) + 1

    def send_motion_request(self, request_text):
        """å‘é€è¿åŠ¨è¯·æ±‚ï¼ˆä¿®å¤futureé—®é¢˜ç‰ˆï¼‰"""
        if not isinstance(request_text, str):
            request_text = json.dumps(request_text)

        # ä¿ç•™åŸæœ‰å®¢æˆ·ç«¯åˆå§‹åŒ–é€»è¾‘
        client = self.create_client(ChatGPT, '/ChatGPT_function_call_service')
        max_retries = 3
        retry_count = 0
        delay_time = 0.5
        while not client.wait_for_service(timeout_sec=delay_time) and retry_count < max_retries:
            self.get_logger().info(f"Service not available, retrying... ({retry_count + 1}/{max_retries})")
            retry_count += 1
            delay_time *= 2

        if retry_count == max_retries:
            self.get_logger().error("Max retries reached. Service call failed.")
            return

        request = ChatGPT.Request()
        request.request_text = request_text
        
        # å…³é”®ä¿®æ”¹ç‚¹ï¼šç»‘å®šæ•°æ®åˆ°futureå¯¹è±¡
        future = client.call_async(request)
        future.task_data = {  # æ–°å¢æ­¤è¡Œ
            "request_text": request_text,  # ç›´æ¥å­˜å‚¨å­—ç¬¦ä¸²
            "timestamp": time.time()
        }
        
        future.add_done_callback(self.function_call_response_callback)

    def state_listener_callback(self, msg):
        self.get_logger().debug(f"model node get current State:{msg}")

    def publish_string(self, string_to_send, publisher_to_use):
        msg = String()
        msg.data = string_to_send
        publisher_to_use.publish(msg)
        self.get_logger().info(f"Topic: {publisher_to_use.topic_name}\nMessage published: {msg.data}")

    def add_message_to_history(self, role, content="null", function_call=None, name=None):
        message_element_object = {
            "role": role,
            "content": content,
        }
        if name is not None:
            message_element_object["name"] = name
        if function_call is not None:
            message_element_object["function_call"] = function_call
        config.chat_history.append(message_element_object)
        if len(config.chat_history) > config.chat_history_max_length:
            self.get_logger().info(f"Chat history is too long, popping the oldest message: {config.chat_history[0]}")
            config.chat_history.pop(0)
        return config.chat_history

    def generate_chatgpt_response(self, messages_input):
        response = self.client.chat.completions.create(
            model=config.openai_model,
            messages=messages_input,
            functions=config.robot_functions_list,
            function_call="auto",
        )
        return response

    def process_chatgpt_response(self, messages_input):
        chatgpt_response = self.generate_chatgpt_response(messages_input)
        choice, message, content, function_call, function_flag = self.get_response_information(chatgpt_response)
        self.add_message_to_history(role="assistant", content=content, function_call=function_call)
        self.write_chat_history_to_json()
        if function_flag == 1:  # å¦‚æœå“åº”ç±»å‹æ˜¯å‡½æ•°è°ƒç”¨
            self.publish_string("function_call", self.llm_response_type_publisher)
            self.get_logger().info("STATE: function_execution")
            self.function_call(function_call)  # è°ƒç”¨æœºå™¨äººå‡½æ•°
        else:  # å¦‚æœå“åº”ç±»å‹ä¸æ˜¯å‡½æ•°è°ƒç”¨
            self.publish_string("feedback_for_user", self.llm_response_type_publisher)
            self.get_logger().info("STATE: feedback_for_user")
            self.publish_string(content, self.llm_feedback_publisher)

    def llm_callback(self, msg):
        self.get_logger().info("STATE: model_processing")
        self.get_logger().info(f"Input message received: {msg.data}")
        user_prompt = msg.data
        self.add_message_to_history("user", user_prompt)
        self.process_chatgpt_response(config.chat_history)

    def get_response_information(self, chatgpt_response):
        if isinstance(chatgpt_response, dict):  # å¦‚æœæ˜¯æ—§ç‰ˆ SDK è¿”å›çš„å­—å…¸
            choice = chatgpt_response["choices"][0]["message"]
            content = choice.get("content")
            function_call = choice.get("function_call", None)
        else:  # å¦‚æœæ˜¯æ–°ç‰ˆ SDK è¿”å›çš„ç±»å®ä¾‹
            choice = chatgpt_response.choices[0]
            message = choice.message
            content = getattr(message, "content", None)
            function_call = getattr(message, "function_call", None)
        if function_call:
            function_call = {
                "name": function_call.name,
                "arguments": function_call.arguments
            }
        function_flag = 0 if content is not None else 1
        self.get_logger().info(f"Get message from OpenAI: {choice}, type: {type(choice)}")
        self.get_logger().info(f"Get content from OpenAI: {content}, type: {type(content)}")
        self.get_logger().info(f"Get function call from OpenAI: {function_call}, type: {type(function_call)}")
        return choice, message, content, function_call, function_flag

    def write_chat_history_to_json(self):
        try:
            json_data = json.dumps(config.chat_history)
            with open(self.chat_history_file, "w", encoding="utf-8") as file:
                file.write(json_data)
            self.get_logger().info("Chat history has been written to JSON")
            return True
        except IOError as error:
            self.get_logger().error(f"Error writing chat history to JSON: {error}")
            return False

    def function_call(self, function_call_input):
        if not self.function_call_client.wait_for_service(timeout_sec=3.0):
            self.get_logger().error("ChatGPT_function_call_service æœªå°±ç»ªï¼Œè°ƒç”¨å¤±è´¥")
            return
        
        try:
            parsed_arguments = self._parse_function_arguments(function_call_input)
            validated_request = {
                "name": function_call_input["name"],
                "arguments": parsed_arguments
            }
            function_call_input_str = json.dumps(
                validated_request,
                ensure_ascii=False,
                separators=(",", ":")
            )
            self.function_name = validated_request["name"]
            self.function_call_request.request_text = function_call_input_str
            self.get_logger().info(
                f"Request for ChatGPT_function_call_service: {self.function_call_request.request_text}",
                throttle_duration_sec=1
            )
            future = self.function_call_client.call_async(self.function_call_request)
            future.task_data = {  # æ–°å¢æ­¤è¡Œ
                "request_text": function_call_input_str,  # ç›´æ¥å­˜å‚¨å­—ç¬¦ä¸²
                "timestamp": time.time()
            }
            future.add_done_callback(self.function_call_response_callback)
        
        except Exception as e:
            self.get_logger().error(f"å‡½æ•°è°ƒç”¨é¢„å¤„ç†å¤±è´¥: {str(e)}")

    def _parse_function_arguments(self, function_call_input):
        raw_arguments = function_call_input.get("arguments", {})
        if isinstance(raw_arguments, str):
            try:
                parsed_args = json.loads(raw_arguments)
            except json.JSONDecodeError:
                self.get_logger().error("æ— æ³•è§£æçš„argumentsæ ¼å¼")
                return {}
        else:
            parsed_args = raw_arguments

        required_fields = ["robot_name", "duration", "linear_x", "angular_z"]
        for field in required_fields:
            if field not in parsed_args:
                self.get_logger().error(f"Missing required field: {field}")
                return {}

        try:
            robot_name = str(parsed_args["robot_name"])
            duration = float(parsed_args["duration"])
            linear_x = float(parsed_args["linear_x"])
            angular_z = float(parsed_args["angular_z"])

            # å‚æ•°èŒƒå›´æ ¡éªŒ
            if duration <= 0:
                raise ValueError("Duration must be positive.")
            if abs(linear_x) > 1.0 or abs(angular_z) > 1.0:
                raise ValueError("Linear and angular values must be within [-1.0, 1.0].")

            return {
                "robot_name": robot_name,
                "duration": duration,
                "linear_x": linear_x,
                "angular_z": angular_z
            }
        except (KeyError, ValueError) as e:
            self.get_logger().error(f"Invalid arguments: {str(e)}")
            return {}

    def function_call_response_callback(self, future):
        """æœåŠ¡å“åº”å›è°ƒï¼ˆçº¿ç¨‹å®‰å…¨ä¿®å¤ç‰ˆï¼‰"""
        # æ–°å¢futureæœ‰æ•ˆæ€§æ ¡éªŒ
        if not hasattr(future, 'task_data'):
            self.get_logger().error("Invalid future object")
            self.get_logger().error(f"Future object: {future}")
            return

        # ä»futureç›´æ¥è·å–æ•°æ®ï¼ˆå…³é”®ä¿®æ”¹ç‚¹ï¼‰
        task_data = future.task_data
        try:
            request_text = task_data["request_text"]
        except KeyError:
            self.get_logger().error("Missing request_text in task_data")
            return

        # ä¿æŒåŸæœ‰å“åº”å¤„ç†é€»è¾‘
        try:
            response = future.result()
            if not hasattr(response, 'response_text'):
                raise ValueError("æ— æ•ˆçš„æœåŠ¡å“åº”æ ¼å¼")
        except Exception as e:
            self.get_logger().error(f"æœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}")
            return

        # ä¼˜åŒ–é˜Ÿåˆ—å¤„ç†é€»è¾‘
        if "Robot is busy" in response.response_text:
            with self.motion_queue_lock:
                if request_text in self.motion_queue:
                    self.get_logger().debug("é‡å¤ä»»åŠ¡å·²å­˜åœ¨")
                    return
                if len(self.motion_queue) >= 10:
                    self.get_logger().warn("ä»»åŠ¡é˜Ÿåˆ—å·²æ»¡ï¼Œç§»é™¤æœ€æ—©çš„ä»»åŠ¡")
                    self.motion_queue.popleft()
                self.motion_queue.append(request_text)  # ä½¿ç”¨ç»‘å®šæ•°æ®
            return

        # ä¿æŒåŸæœ‰æ­£å¸¸å¤„ç†æµç¨‹
        try:
            self.add_message_to_history(
                role="function", 
                name=self.function_name,
                content=response.response_text
            )
            self.process_chatgpt_response(config.chat_history)
        except json.JSONDecodeError:
            self.get_logger().error("å“åº”JSONè§£æå¤±è´¥")

def main(args=None):
    rclpy.init(args=args)
    chatgpt = ChatGPTNode()
    rclpy.spin(chatgpt)
    rclpy.shutdown()

if __name__ == "__main__":
    main()
